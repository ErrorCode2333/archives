【1】
颜色模式
OpenGL颜色模式一共有两个：RGB（RGBA）模式和颜色表索引模式。在RGB模式下，所有的颜色定义全用R、G、B三个值来表示，有时也加上Alpha值（与透明度有关），即RGBA模式。在颜色表模式下，每一个象素的颜色是用颜色表中的某个颜色索引值表示，而这个索引值指向了相应的R、G、B值。这样的一个表成为颜色映射（Color Map）。

【2】

颜色缓冲区
         OpenGL在绘制图元时，先是在一个缓冲区中完成渲染，然后再把渲染结果交换到屏幕上。我们把这两个缓冲区称为前颜色缓冲区（屏幕）和后颜色缓冲区。在默认情况下，OpenGL命令是在后颜色缓冲区进行渲染的。当然，也可以直接在前颜色缓冲区中进行渲染。

         若要在前颜色缓冲区中进行渲染，第一种方法是直接告诉OpenGL希望在前颜色缓冲区中进行绘图，可以调用下面这个函数来实现这个目的：

         void glDrawBuffer（Glenum mode）；

         如果参数mode指定为GL_FRONT，OpenGL就会在前颜色缓冲区中进行渲染；

         如果参数mode指定为GL_BACK，那么渲染将在后颜色缓冲区中进行。


         在前颜色缓冲区进行渲染的第二种方法是在OpenGL被初始化时简单地不要求进行双缓冲区渲染。进行单缓冲区渲染时，如果希望把渲染结果实际绘制到屏幕上，需要调用glFlush（）或glFinsh（），这点非常重要。

         
         OpenGL实现除了支持单纯的前颜色缓冲区和后颜色缓冲区之外，还支持其他模式，如用于立体渲染的左和右缓冲区以及辅助缓冲区。
      

深度缓冲区
         与颜色缓冲区不同的是，深度缓冲区中所填充的是深度值而不是颜色值。

         为了启用深度缓冲区进行深度测试，只需要调用：

         glEnable（GL_DEPTH_TEST）；

         另外，即使深度缓冲区未被启用，如果深度缓冲区被创建，OpenGL也会把所有写入到颜色缓冲区的颜色片段对应的深度值写入到深度缓冲区中。但是，如果我们希望在进行深度测试时临时禁止把值写入到深度缓冲区，我们可以使用函数：

         void glDepthMask（GLboolean mask）；

         把GL_FALSE作为参数，经禁止写入深度值，但并不禁止用已经写入到深度缓冲区的值进行深度测试。

         把GL_TRUE作为参数，可以重新启用深度缓冲区的写入。同时，这也是默认的设置。

 

裁剪测试
         OpenGL允许在窗口中指定一个裁剪矩形，让渲染只在这个区域内进行。在默认情况下，裁剪矩形就是窗口的大小，不会进行裁剪测试。我们可以打开裁剪测试：

         glEnable（GL_SCISSOR_TEST）；

         在窗口内部执行渲染的那个区域称为裁剪框（scissor box），它使用下面这个函数以窗口坐标的形式指定：

         void glScissor（Glint x， Glint y， GLsizei width， GLsizei height）；// 左下原点

【3】

OPENGL坐标系可分为：世界坐标系和当前绘图坐标系。

世界坐标系以屏幕中心为原点(0, 0, 0)。你面对屏幕，你的右边是x正轴，上面是y正轴，屏幕指向你的为z正轴。长度单位这样来定： 窗口范围按此单位恰好是(-1,-1)到(1,1)。

当前绘图坐标系是 绘制物体时的坐标系。程序刚初始化时，世界坐标系和当前绘图坐标系是重合的。当用glTranslatef()，glScalef(), glRotatef()对当前绘图坐标系进行平移、伸缩、旋转变换之后， 世界坐标系和当前绘图坐标系不再重合。改变以后，再用glVertex3f()等绘图函数绘图时，都是在当前绘图坐标系进行绘图，所有的函数参数也都是相 对当前绘图坐标系来讲的。


屏幕坐标系：左下角为原点。？

【4】
坐标系旋转：
进行旋转操作时需要指定的角度θ的方向则由右手法则来决定，即右手握拳，大拇指直
向某个坐标轴的正方向，那么其余四指指向的方向即为该坐标轴上的θ角的正方向（即θ
角增加的方向）

【5】
在实际编程中，为了保存几何变换和投影变换的操作，
OpenGL维护两个栈，
即投影变换栈和几何变换栈。
栈中保存的元素即为投影变换和几何变换的变换矩阵。使用下面的函数可以在两个栈之间进行切换：
  
切换当前操作的栈为投影变换栈：
glMatrixMode(GL_PROJECTION); 
切换当前操作的栈为几何变换栈：
glMatrixMode(GL_MODELVIEW);  
此外，下面的函数可以清除当前操作的栈中的内容，并向栈中压入一个单位矩阵：
 
glLoadIdentity();

屏幕坐标系投影坐标 = 投影矩阵X变换矩阵X世界坐标

【6】

glPushMatrix(); 
// 
保存当前坐标系
 
glPopMatrix();  
// 
恢复当前坐标系
  
在调用几何变换操作时，
OpenGL将该几何变换操作的变换矩阵与当前栈的栈顶元素相乘（！左乘），得到一个新 
的矩阵并将其作为几何变换栈的栈顶。

例子：
glMatrixMode(GL_PROJECTION); 
glLoadIdentity();  
gluPerspective(30.0, aspect, 1.0, 50.0); 
glMatrixMode(GL_MODELVIEW); 
glPushMatrix();  
glTranslatef(0.0, 0.0, -20.0); 
glPushMatrix();  
glTranslatef(0.0, 1.0, 0.0); 
myWireCylinder(1.0, 2.0, 12); 
glTranslatef(0.0, 1.0, 0.0); 
glRotatef(-90.0, 1.0, 0.0, 0.0); 
glutWireCone(1.0, 2.0, 12, 3); 
glPopMatrix();  
glTranslatef(0.0, -1.0, 0.0); 
myWireCylinder(1.0, 2.0, 12); 
glPopMatrix();

【7】投影
1 正射投影(Orthographic Projection)
正射投影，又叫平行投影。这种投影的视景体是一个矩形的平行管道，也就是一个长方体，如图所示。正射投影的最大一个特点是无论物体距离相机多远，投影后的物体大小尺寸不变。这种投影通常用在建筑蓝图绘制和计算机辅助设计等方面，这些行业要求投影后的物体尺寸及相互间的角度不变，以便施工或制造时物体比例大小正确。

此种模式下，不需要设定照相机位置、方向以及视点的位置，也就是说不需要gluLookAt函数。

    OpenGL正射投影函数共有两个。

    一个函数是：

void glOrtho(GLdouble left,GLdouble right,GLdouble bottom,GLdouble top, GLdouble near,GLdouble far)

它创建一个平行视景体。实际上这个函数的操作是创建一个正射投影矩阵，并且用这个矩阵乘以当前矩阵。其中近裁剪平面是一个矩形，矩形左下角点三维空间坐标是(left,bottom,-near)，右上角点是(right,top,-near)；远裁剪平面也是一个矩形，左下角点空间坐标是(left,bottom,-far)，右上角点是(right,top,-far)。所有的near和far值同时为正或同时为负。如果没有其他变换，正射投影的方向平行于Z轴，且视点朝向Z负轴。这意味着物体在视点前面时far和near都为负值，物体在视点后面时far和near都为正值。

2 透视投影(Perspective Projection)

透视投影符合人们心理习惯，即离视点近的物体大，离视点远的物体小，远到极点即为消失，成为灭点。它的视景体类似于一个顶部和底部都被切除掉的棱椎，也就是棱台。这个投影通常用于动画、视觉仿真以及其它许多具有真实性反映的方面。OpenGL透视投影函数也有两个，其中函数glFrustum()。

此种情况下，需要用gluLookAt设定照相机位置、照相机方向(一般设置为(0,1,0))、以及视点位置。

    这个函数原型为：

void glFrustum(GLdouble left,GLdouble Right,GLdouble bottom,GLdouble top, GLdouble near,GLdouble far);

它创建一个透视视景体。其操作是创建一个透视投影矩阵，并且用这个矩阵乘以当前矩阵。这个函数的参数只定义近裁剪平面的左下角点和右上角点的三维空间坐标，即(left,bottom,-near)和(right,top,-near)；最后一个参数far是远裁剪平面的Z负值，其左下角点和右上角点空间坐标由函数根据透视投影原理自动生成。near和far表示离视点的远近，它们总为正值。

    另一个函数是：

void gluPerspective(GLdouble fovy,GLdouble aspect,GLdouble zNear, GLdouble zFar);

它也创建一个对称透视视景体，但它的参数定义于前面的不同，如图所示。其操作是创建一个对称的透视投影矩阵，并且用这个矩阵乘以当前矩阵。参数fovy定义视野在X-Z平面的角度，范围是[0.0,180.0]；参数aspect是投影平面宽度与高度的比率；参数zNear和Far分别是远近裁剪面到眼睛的距离，它们总为正值。

////////////////////////
透视投影的聚焦点，在观察坐标系的原点（也就是我们gluLooKAt的指定的观察点），观察平面就在近平面这里，也就是zNear指定的地方，　观察平面是平行于观察坐标系的（Ｘ，Ｙ），因此我们指定ｚ轴，也就指定它的位置．　

void gluLookAt(GLdouble eyex,GLdouble eyey,GLdouble eyez,
                                   GLdouble centerx,GLdouble centery,GLdouble centerz,
                                   GLdouble upx,GLdouble upy,GLdouble upz);
该函数定义一个视图矩阵，并与当前矩阵相乘。
第一组eyex, eyey,eyez 相机在世界坐标的位置
第二组centerx,centery,centerz 相机镜头对准的物体在世界坐标的位置
第三组upx,upy,upz 相机向上的方向在世界坐标中的方向
你把相机想象成为你自己的脑袋：
第一组数据就是脑袋的位置
第二组数据就是眼睛看的物体的位置
第三组就是头顶朝向的方向（因为你可以歪着头看同一个物体）。


/**************************************************************/
视口就是窗口内部用于绘制裁剪区域的客户端区域。视口简单的将裁剪区域映射到窗口中的一个区域。


/**************************************************************/
纹理 :
计算机图形学中，纹理指的是一张表示物体表面细节的位图。

*blend
把将要画上去的颜色称为“源颜色”，把原来的颜色称为“目标颜色”。OpenGL 会把源颜色和目标颜色各自取出，并乘以一个系数（源颜色乘以的系数称为“源因子”，目标颜色乘以的系数称为“目标因子”），然后相加，这样就得到了新的颜色。

【1】
void glVertexPointer(GLint size, GLenum type, GLsizei stride, const GLvoid * pointer)
参数：
size：指定了每个顶点对应的坐标个数，只能是2,3,4中的一个，默认值是4
type：指定了数组中每个顶点坐标的数据类型，可取常量:GL_BYTE, GL_SHORT,GL_FIXED,GL_FLOAT;
stride:指定了连续顶点间的字节排列方式，如果为0，数组中的顶点就会被认为是按照紧凑方式排列的，默认值为0
pointer:制订了数组中第一个顶点的首地址，默认值为0，对于我们的android，大家可以不用去管什么地址的，一般给一个IntBuffer就可以了。
描述：
当开始render的时候，glVertexPointer 用一个数组指定了每个顶点的坐标，

【2】
函数原型：GL_APICALL void GL_APIENTRY glDrawArrays (GLenum mode, GLint first, GLsizei count);
相似功能的函数是 glDrawElements。
参数说明：
mode，绘制方式，OpenGL2.0以后提供以下参数：GL_POINTS、GL_LINES、GL_LINE_LOOP、GL_LINE_STRIP、GL_TRIANGLES、GL_TRIANGLE_STRIP、GL_TRIANGLE_FAN。
first，从数组缓存中的哪一位开始绘制，一般为0。
count，数组中顶点的数量。

【3】
索引数组(indices)

glDrawElements函数用于循环调用glArrayElement,但需要定义一个索引的数组

void glDrawElements( GLenum mode, GLsizei count,
GLenum type, const GLvoid *indices）；
其中：
mode指定绘制图元的类型，它应该是下列值之一，GL_POINTS, GL_LINE_STRIP, GL_LINE_LOOP, GL_LINES, GL_TRIANGLE_STRIP, GL_TRIANGLE_FAN, GL_TRIANGLES, GL_QUAD_STRIP, GL_QUADS, and GL_POLYGON.
count为绘制图元的数量乘上一个图元的顶点数。
type为索引值的类型，只能是下列值之一：GL_UNSIGNED_BYTE, GL_UNSIGNED_SHORT, or GL_UNSIGNED_INT。
indices：指向索引存贮位置的指针。

示例5

void drawTwoLineWithArray2()
{
    GLfloat data[]=  {1.0, 0.0, 0.0,25.0,25.0,
                      1.0, 0.0, 0.0,100.0,100.0,
                      0.0, 1.0, 0.0,120.0,120.0,
                      0.0, 1.0, 0.0,200.0,200.0};
    GLubyte index[]= {0,1,2,3} ;
    

    glEnableClientState(GL_VERTEX_ARRAY);
    glEnableClientState(GL_COLOR_ARRAY);
    glColorPointer(3,GL_FLOAT,5*sizeof(GLfloat),&data[0]);
    glVertexPointer(2,GL_FLOAT,5*sizeof(GLfloat),&data[3]);
    
    glDrawElements(GL_LINES,4,GL_UNSIGNED_BYTE,index);
    
}

【4】glArrayElement 函数指定用来呈现一个顶点的数组元素。
【5】
void glGenBuffers(GLsizei n, GLuint *buffers);
//在buffers数组中返回当前n个未使用的名称，表示缓冲区对象
GLboolean glIsBuffer(GLuint buffer);
//判断是否是缓冲区对象

code：
glGenBuffers(3,buffers); //创建3块buffer
glBindBuffer(GL_ARRAY_BUFFER, buffers[0]);//先将第一块buffer绑定到当前
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices),vertices, GL_STATIC_DRAW);//将顶点数组放入这个buffer
glVertexPointer(3, GL_FLOAT, 0, 0);//指定这个buffer的格式，3代表3个float值为一个顶点，第一个0代表跨度，这里数值是紧密排列的，最后一个是数组的入口地址*point，由于已经将buffer绑定到当前，所以填0
glEnableClientState(GL_VERTEX_ARRAY);//使用这个buffer，一定不要忘记
glBindBuffer(GL_ARRAY_BUFFER, buffers[1]);//这里开始是将第二块buffer绑定到当前，上传color数据，和顶点的做法一样，不多赘述了
glBufferData(GL_ARRAY_BUFFER, sizeof(colors), colors, GL_STATIC_DRAW);
glColorPointer(3, GL_FLOAT, 0, 0);
glEnableClientState(GL_COLOR_ARRAY);

glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, buffers[2]);//将第三块buffer绑定到当前，用于上传存储顶点索引的数据，因此BUFFER类型变更为GL_ELEMENT_ARRAY_BUFFER
glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(frontIndices),frontIndices,GL_STATIC_DRAW);//同样是对buffer进行填充
经过以上代码，某个图元的顶点位置以及颜色和索引信息就已经放入GPU了，接下来只要在display函数中，调用glDrawElements就可以绘制了
glDrawElements(GL_QUADS, 4, GL_UNSIGNED_SHORT, BUFFER_OFFSET(0));//由于索引定义的是4个点，因此第一个参数mode是四边形，第二个是索引的size，第三个是索引的数据类型。注意！glDrawElements只接受3种数据类型，当时我因为指定索引类型为GL_INT导致无法绘制被卡住了N长时间，坑人啊！最后一个是指针地址，由于当前绑定的buffer是buffer[2]，因此填0即可。
最后别忘记了，VBO是需要glew库支持的，所以要记得在头文件中添加

【6】
著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：hoodlum1980
链接：https://www.zhihu.com/question/30095978/answer/67147426
来源：知乎

vao，就是顶点数组。绑定，就是指定你当前要操作的对象。（opengl 只有一个当前对象，把它想象成 current object，你的后续操作，都是在对这个 object 进行。你可以拥有很多个 object，然后用 bind 来指定 current object，所以你后面的操作，就不需要点名你在操作谁了）。用绑定，可以随时的在多个 object 之间切换当前对象。genXXXXX，会返回一个整数形式的 ID（handle），这个数是这个对象的 ID，有如身份证号。但是由于我们还没有给这个对象赋实际数据，所以我们可以设想，这类似于像 opengl 声明了这个对象的存在（申领一个 unique id）。好比有了一个箭头，但是箭头另一端没有挂接数据。vao，就是一堆顶点，组成的一个数组。那么我们就得知道这里的顶点是什么，所谓顶点，就是一个点，对每个这样的点，上面挂接了多个数据（通常都是1~4维向量），这些数据成为属性。我们提到具体哪个属性时，用索引值来标识。比如说，坐标，就是顶点的一个必有的属性，通常是第一个属性，所以：顶点.属性[0] --> [x,y,z]；除了坐标，还可以有颜色 [r,g,b], 纹理坐标 [u,v] ，法线方向等。总之，顶点，有多个向量形式的属性，多个顶点，组成一个 va。所以一个 va，想象成就是一个箭头，指向了具有多个格子的指针数组，每一个格子又有一个箭头，指向一堆向量形式的属性（依次属于每个顶点）。对一个 vao，申领了 ID 以后，只是一个悬空的箭头，所以要给这个 vao 设置属性，通常至少要有坐标这个属性。所以需要先创建一个 buffer，把数据传递到 buffer，genBuffer,  给 buffer 申领一个 id，BufferData ，给 buffer 赋值（向 buffer 中拷贝数据）。这一步可能才是（opengl / 显卡）实际分配内存。VertextAttribPoint，把这个 buffer 设置为某个 vao 的某个属性。这样，vao 的某个属性就有了实际数据。


当然，这些属性具体是什么含义，主要取决于你怎么（在 shader 中）使用它们。这些属性，在 pipeline 中，送给 vertext shader 作为可使用的输入数据。比如说，vertex shader 中可以用一个4x4 计算出来的矩阵，乘以顶点的属性0，把模型坐标变换到窗口的客户区坐标。所以当你对某个 vao 设置完属性以后，opengl/显卡就已经知道了这些信息，所以除非你需要修改，否则你只需要做一次就够了，然后你可以把它切换成非 current 对象。需要的时候随时可以 bind 到前台使用。当然了，实际存储大量数据和需要较大内存的，是 vbo，也就是 buffer。一个 buffer 被挂接给某个 vao 的某个属性。我们就可以因此知道这个顶点的属性（数据）。当你不需要对象的时候，要记得 delete 。申领的 id，只是交还给系统（标示为可以分配给用户）。对于 buffer，会释放 buffer 占用的内存。


VAO是一个状态集，标记的是当前GL_ARRAY_BUFFER Target的数据状态。而单纯的VBO只是一块Data，并没有状态。这句话的意思是，在你的程序中你可以gen N个 VAO，也可以gen N个VBO，但是，起作用的只是你bind到Target上的那一个。glBindVertexArray 为什么没有Target参数，因为Target只有一种。

【7】
所谓GPU的渲染管线，听起来好像很高深的样子，其实我们可以把它理解为一个流程，就是我们告诉GPU一堆数据，最后得出来一副二维图像，而这些数据就包括了”视点、三维物体、光源、照明模型、纹理”等元素。 
在各种图形学的书中，渲染管线主要分为三个阶段：应用程序阶段、几何阶段、光栅阶段。 

2，几何阶段。 
主要负责顶点坐标变换、光照、裁剪、投影以及屏幕映射，改阶段基于GPU进行运算，在该阶段的末端得到了经过变换和投影之后的顶点坐标、颜色、以及纹理坐标。简而言之，几何阶段的主要工作就是“变换三维顶点坐标”和“光照计算”。 

3，光栅化阶段。 
经过上面的步骤之后，我们得到了每个点的屏幕坐标值，和我们需要绘制的图元，
给像素赋予颜色的阶段称为Pixel Operation，是在更新帧缓存之前，执行最后一系列针对每个片段的操作，其目的是计算出每个像素的颜色值。在这个阶段，被遮挡的面通过一个被称为深度测试的过程消除。 
pixel operation包含下面这些流程： 
（1）消除遮挡面； 
（2）Texture operation，纹理操作，根据像素的纹理坐标，查询对应的纹理值； 
（3）Blending，通常称为alpha blending，根据目前已经画好的颜色，与正在计算的颜色的alpha值混合，形成新的颜色。 
（4）Filtering，将正在计算的颜色经过某种滤镜后输出。 
该阶段之后，像素的颜色值被写入帧缓存中。 

【8】glBindBuffer激活缓冲区，指定当前缓冲对象，从而使接下的操作都是对该对象进行的。GL_ARRAY_BUFFER表示顶点数据，GL_ELEMENT_ARRAY_BUFFER表示索引数据。


【9】
    纹理滤镜

    在纹理映射的过程中，如果图元的大小不等于纹理的大小，OpenGL便会对纹理进行缩放以适应图元的尺寸。我们可以通过设置纹理滤镜来决定OpenGL对某个纹理采用的放大、缩小的算法。

    调用glTexParameter来设置纹理滤镜。如：

    glTexParameteri(GL_TEXTURE_2D,GL_TEXTURE_MAG_FILETER,
MagFilter);//设置放大滤镜
    glTexParameteri(GL_TEXTURE_2D,GL_TEXTURE_MIN_FILTER, 
MinFilter); //设置缩小滤镜

   上述调用中，第一个参数表明是针对何种纹理进行设置，第二个参数表示要设置放大滤镜还是缩小滤镜。第三个参数表示使用的滤镜。可以为下面的值之一：


   6.4.1 纹理坐标

    要使用当前的纹理绘制图元，我们必须在绘制每个顶点之前为该顶点指定纹理坐标。只需调用

    glTexCoord2d(s:Double;t:Double);

    函数即可。其中，s、t是对于2D纹理而言的s、t坐标。

图6.4-1 纹理坐标

    对于任何纹理，无论纹理的真正大小如何，其顶端（左上角）的纹理坐标恒为(0,0),右下角的纹理坐标恒为(1,1)。也就是说，纹理坐标应是一个介于0到1之间的一个小数。


【10】
DIB意思就是Device-Independent Bitmap(设备无关位图)。

【11】
I.纹理过滤:
当三维空间里面的多边形经过坐标变换、投影、光栅化等过程，变成二维屏幕上的一组象素的时候，对每个象素需要到相应纹理图像中进行采样，这个过程就称为纹理过滤。

II.纹理过滤通常分为2种情况： 
a) 纹理被缩小 GL_TEXTURE_MIN_FILTER 
比如说一个8 x 8的纹理贴到一个平行于xy平面的正方形上，最后该正方形在屏幕上只占4 x 4的象素矩阵，这种情况下一个象素对应着多个纹理单元。
b) 纹理被放大 GL_TEXTURE_MAG_FILTER 
纹理被放大这种情况刚好跟上面相反，假如我们放大该正方形，最后正方形在屏幕上占了一个16 x 16的象素矩阵，这样就变成一个纹理单元对应着多个象素。

III.几种不同的纹理过滤方式： 
1.最近点采样 GL_NEAREST
2.线性纹理过滤(双线性过滤)GL_LINEAR
3.mipmap纹理过滤(三线性过滤) GL_LINEAR_MIPMAP_LINEAR
4.各向异性过滤


纹理的寻址模式：

可以大于[0.0， 1.0]的区间，可以重复纹理

纹理优化：
1.色深优化
例如RGBA8888 -> RGBA5551

2.拼图优化
小图合并为大纹理，在渲染中，使用散图意味着增加切换纹理的开销，如果图片很多，那么纹理的频繁切换会导致你的游戏FPS大大降低。而采用拼合图，你将只需要很少次设定使用纹理，就可以渲染出大量的图形，大大减少了切换纹理的开销。

3.骨骼动画
    骨骼动画的制做流程一般为:
    1.通过设定基础的骨骼点并构建出骨架关系树。将人物按骨架树划分成各个身体部件。
    2.按照划定的身体部件来制做人物或动物图片，当然一般是将整个人物或动物图片进行切图命名图块来取得。
    3.将身体部件图块绑定到相应的骨骼上。
    4.设定关键帧，编辑关键帧上所有骨骼的位置及缩放与旋转变化。
    5.播放动画，使骨骼按照时间进行两个关键帧间的变化插值计算，使用插值后的结果来影响身体部件图块绘制时的顶点位置。

4.批次
即Batch，也是游戏引擎中一个重要的优化指标。它指的是一次渲染凋用。显而易见，渲染的次数越少则游戏的运行效率越高。
Cocos2d-x是明显懂得这个道理的，举个最明显的栗子。咱们之前讲过的“深入分析Cocos2d-x中的图字原理”中有一个类CCSpriteBatchNode，它就是为了降低渲染批次而创造的一个专门管理精灵的类。这么说吧，我们渲染一段话“你吃了么？”，这里面有五个字，所以要创建五个CCSprite来对应这五个这的图块绘制，但绘制时我们如果一个一个的绘制，那是很恐怖的一件事，你的游戏写的字如果很多，你就可以放弃干其它的事情了，因为渲染调用次数太多而导致效率低下。为了提高渲染效率，专门的类CCSpriteBatchNode就出现了！他在一次渲染中把这所有的CCSprite都绘制了出来。所以，我想说，如果你要提高你的渲染效率，好好分析分析怎么使用CCSpriteBatchNode，将你的游戏中的大量图片精灵尽可能的放在CCSpriteBatchNode中来渲染，一定可以提高FPS的。

5.使用压缩纹理
IOS设备用的是PowerVR显示芯片，而PVR格式可以被该显示芯片直接读取，在之前的博文里有分析过PVR格式纹理的源码，大家可以从中了解到PVR格式所支持的多种压缩格式。我们在开发IOS游戏时，也应该尽量使用合适的压缩格式的图片。如“pvr.ccz”。

////////////////////////////////////////////////////////////////////////////////////////////////////
1、2d游戏最占内存的无疑是图片资源。

2、cocos2d-x不同平台读取纹理的机制不同。ios下面使用CGImage。android和windows下是直接调用png库。我测试了下，使用png库直接读取png会比CGImage还要节约1mb左右内存（图片所占内存4mb）但是速度要比CGImage慢一倍。时间和空间如何取舍就看实际情况了。不过最佳的选择似乎是pvr（即使android版本，即使不使用pvrtc4）。

3、一般来说，我们可以直接使用  w * h * bpp得到一张纹理所占的内存，比如一张1024*1024格式为argb8888，那么他所占的内存就是1024*1024*4=4mb。之前看到有博客提到jpg会开辟3倍与此的内存（先转换为png，然后解析png），但是新的ios系统似乎没有这个问题。jpg与png所消耗的内存几乎相同，并且jpg解析速度更快（几乎都是4mb解析+4mb纹理数据，而jpg解析时间是png的一半），但是这样反而很怪异，因为jpg是没有透明色的，一个像素最多3字节，而png一个像素4字节，jpg纹理应该占用内存更小才对，后来看了下cocos2d的ios加载图片的代码，它把所有纹理转换成rgba8888格式，所以无论是jpg还是png，占用的都是4字节。正因cocos2d对其他纹理支持不够好，pvr才会显得那么高效。

4、pvr格式可以被显卡所认可，而不需要开辟临时内存来读取，所以即便同为argb8888格式的图片，pvr也会比png有效率，虽然不会节约程序稳定运行时的内存，但是会避免加载大量图片时的内存暴涨。  并且如果是ios设备的话，可以使用pvrtc4格式的图片，这个格式相当于windows下的dds图片，是可以被显卡直接支持的。它是有损压缩，一个像素只占4位，不过如果不是有渐变半透明色的话，一般效果可以接受，而其节约的内存和cpu时间非常非常显著。

5、pvr也不是万金油。android设备下虽然可以使用pvr格式，但是不能使用pvrtc4，希望通过pvr像ios设备上一样真正减少游戏内存是不太可行的。

6、pvr.ccz其实就是pvr图片zip打包下，程序读的时候还是先解压出pvr资源，然后再读取pvr。不过由于压缩下可以极大的减小图片体积，所以虽然多了解压过程也不会有特别多的cpu消耗。

7、一张jpg图片实际加载过程内存消耗，以一张1024*1024 argb8888 500k的jpg图片为例： a.读取图片文件(消耗图片大小内存，500k)     b、解析jpg数据（cgimage, 4mb） c、释放500k的图片内存    d、opengl纹理数据(4mb)    e、释放cgimage的4mb内存。      注意，这个过程不是必然的顺序执行，释放cgimage内存的实际是有系统决定的，会很快，但是不一定是立即执行。  所以内存会瞬间飙升9mb左右，然后减少5mb，稳定到4mb左右

       png图片的加载过程与此相同

       pvr图片可以节约解析图片数据到纹理这一步的消耗。也就是说读取pvr图片资源（等价于解压pvr.ccz到内存，如果是1024*1024 argb8888格式的话，那么图片大小就是4mb，ccz压缩后图片1mb左右）消耗4mb，将pvr图片数据提交给显卡消耗4mb。然后释放文件数据4mb。这么看似乎跟Png从内存占用上相比也不是非常有优势。（注意这里说的pvr是指pvr封装的argb8888，与pvrtc4的性能有天壤之别）

8、由于最终消耗内存的都是纹理数据，所以只要纹理数据格式是一定的，无论图片是什么格式消耗的内存都是一样的。比如使用Png8图片，体积会减少70%，但是内存占用与png24/png32是等价的(读取的时候会内部把调色板还原成真彩色，也就是说，虽然png8是一个像素只占8位，但是读取到内存中的时候会将调色板颜色还原，依然需要开辟1024*1024*4字节的空间存放纹理数据)。 当然有无透明色，cocos2d的处理还是有区别的。如果是无透明色，可以使用png24，那么所需开辟的纹理空间就是3mb。

       这里还有一点需要说明，一般我们处理windows下的dds纹理的时候，都习惯将其按2的整次幂对其，虽然图片内容只有900*900，但是图片大小却是1024*1024。那我们读取这个图片所消耗的内存就是4mb，按2的整次幂对其是有助于提高运行效率的，但是不是非常必须的。ios和android的设备都支持非2的整次幂的纹理。所以如果是png图片，那么它该多大就多大。此时消耗的内存就只有900*900*4=3mb。

9、不要过于迷信所谓的去除alpha通道以节约内存。这个还要实际分析下具体结果。  我测试过（分别用cocos2d-x和鬼火3d引擎），rgba8888和rgb888格式的png图片显示所消耗的内存是一样的。24位图片虽然读取的时候开辟的内存只有3mb（1024*1024*3，注意如果是用CGImage读取的话，那这个值就是4mb），但是glTexImage2D提交给显卡后依然会增加4mb内存。可能跟显卡的数据对齐有关。

     这里我测试还有一个诡异的地方，如果是用pvr的npot图片的话，rgb888要比rgba8888所消耗的内存要小，但是pot图片两者又是一样的（png图片两种情况都是一样的）。可能是powervr显卡有特殊处理。

10、rgb565和rgb5551的图片所消耗的内存是rgba8888的一半，如果没有透明渐变的话，视觉上也看不出什么区别。一些大的背景图可以优先选择这种格式。

11、pvr图片加载速度要比png和jpg快3～5倍（同样1024*1024 argb8888），png消耗的时间可能是700ms左右，但是pvr只需要100ms左右。如果是pvr.ccz压缩下，消耗的时间是200ms左右。可见pvr在加载速度上还是有非常大的优势的。这个应该是因为png和jpg需要把图片数据还原为rgba，但是pvr可以直接把图片数据传递给显卡。pvrtc4的图片是可以被powervr显卡直接支持的。

 

总结下：

1、最终决定图片占用内存的是它的像素格式和大小，与其扩展名无关。png8  png32 jpg pvr只要其像素格式都是argb8888，那么最终图片占用的内存是一样的。

2、如果不是pvrtc4的格式，那么不要扩展成2的整次幂，因为图片越小，占用内存越小

3、单单去除透明通道不会减少图片所消耗的内存，png和jpg图片也无法减少图片体积，所以不推荐rgb888的格式。替代选择rgb565和rgb5551。

5、小心加载图片时临时开辟的纹理数据造成的内存飙高，可以考虑加入内存池，及时的开辟和释放缓冲区。

6、如果是为了减少图片体积可以选择：1、jpg--压缩比最高，质量较好，但是不支持半透明    2、png8--同样图片会比jpg略大一些，使用ImageAlpha进行转换，视觉上几乎看不出差别。    这两种图片格式都可以极大的减少图片体积（减少70%～80%），但是无助于减少内存

7、如果是为了减少内存可以选择：1、没有透明色的图片统一转换为rgb565格式，这个时候无法使用png8了，所以png和pvr.ccz图片大小几乎相同，pvr.ccz速度更快，所以推荐pvr.ccz的rgb565格式    2、如果透明色仅仅是进行关键色标注，而没有渐变混合，那么推荐rgb5551 (r5_a1)的pvr.ccz格式

8、可以考虑写个打包系统，统一把资源文件打包，而不是单个文件用pvr.ccz进行zip压缩，这样可以获得更高的效率。（比如我封装了下暴雪的mpq打包，其读取速度与本地文件读取速度相当，这样就可以获得最佳的读取效率）